{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
   "metadata": {},
   "source": [
    "# Simple bag-of-words baseline for emotion classification (Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
   "metadata": {},
   "source": [
    "Authors: Christine de Kock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this starter notebook, we will take you through the process of emotion classification from text. The notebook was adapted from a notebook for SemEval 2024 Shared Task 1: SemRel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72575e-4b44-48cd-82d0-ee667cd57af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50330823-69db-4204-8646-5847eca34694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dac92ad-3fac-4aa1-aae6-1fe3ad14b1c0",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "The training data consists of a short text and binary labels representing human judgments of the emotions in the text. \n",
    "\n",
    "The data is structured as a CSV file with the following fields:\n",
    "- id: a unique identifier for the sample\n",
    "- text: a sentence or short text\n",
    "- 6 binary fields representing emotion annotations: joy, fear, anger, sadness, surprise\n",
    "\n",
    "The data is multilabel, meaning that more than one of the emotion classes may apply to a given text. \n",
    "\n",
    "Below we will show you how to load and re-format the provided data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fe8cc2-ba47-4240-bdd2-b1d3ea323134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track1_001</td>\n",
       "      <td>None of us has mentioned the incident since.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track1_002</td>\n",
       "      <td>I was 7 and woke up early, so I went to the ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train_track1_003</td>\n",
       "      <td>By that point I felt like someone was stabbing...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train_track1_004</td>\n",
       "      <td>watching her leave with dudes drove me crazy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train_track1_005</td>\n",
       "      <td>`` My eyes widened.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  eng_train_track1_001       None of us has mentioned the incident since.   \n",
       "1  eng_train_track1_002  I was 7 and woke up early, so I went to the ba...   \n",
       "2  eng_train_track1_003  By that point I felt like someone was stabbing...   \n",
       "3  eng_train_track1_004      watching her leave with dudes drove me crazy.   \n",
       "4  eng_train_track1_005                                `` My eyes widened.   \n",
       "\n",
       "   Joy  Fear  Anger  Sadness  Surprise  \n",
       "0    0     1      0        1         1  \n",
       "1    1     0      0        0         0  \n",
       "2    0     1      0        0         0  \n",
       "3    0     1      1        1         0  \n",
       "4    0     1      0        0         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and validation data\n",
    "\n",
    "train = pd.read_csv('data/eng-data/Track 1/eng_track1_train.csv')\n",
    "val = pd.read_csv('data/eng-data/Track 1/eng_track1_dev.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0767ada-33a9-41d5-965a-3e5d096222a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c96e1ed-6cd1-4924-9151-60f3d21a7f4e",
   "metadata": {},
   "source": [
    "## Bag-of-words representation\n",
    "\n",
    "In this tutorial, we use a simple bag-of-words representation to obtain a vector for each text. This vector can then be fed into a machine learning model. More advanced models, including LSTMs and transformers, operate on text directly and to not require the vectorisation step. \n",
    "\n",
    "### Preprocessing \n",
    "We choose to unigrams (that is, individual words) and bigrams (two-word sequences). Texts are lowercased before being vectorised. \n",
    "\n",
    "Further preprocessing steps may include: \n",
    "- stopword removal,\n",
    "- TFIDF normalisation,\n",
    "- lemmatisation / stemming, or\n",
    "- using a different tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb9fe2-b593-4bac-a083-64fd4d9b29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train['text'].str.lower()).toarray()\n",
    "X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "y_train = train[emotions].values\n",
    "y_val = val[emotions].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741f618-f126-4637-b5c7-1e9fb68a3ef3",
   "metadata": {},
   "source": [
    "Finally, we cast the transformed vectors to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f232754-e424-47df-8910-8985e5b89a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train)\n",
    "y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "X_val_t = torch.Tensor(X_val)\n",
    "y_val_t = torch.Tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae3ce2-592c-4d4f-8e02-a7cf8e464503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb9a019d-56e4-4c53-bd85-dfd2308f1bd5",
   "metadata": {},
   "source": [
    "## Characteristics of the data\n",
    "\n",
    "Statistics of the data are printed below. There are 2768 samples in the training data. The input representation consists of 29001 input features and there are 5 output clsees. There is an imbalance in the dataset, with the \"fear\" class being assigned to 58% of samples but the \"anger\" class to only 12%. \n",
    "\n",
    "(Due to the multilabel nature of the data, the percentages do not sum to 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efff5213-7055-48e2-bbf6-22f8cf734b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2768, 29001)\n",
      "Shape of y: (2768, 5)\n",
      "Number of positives per emotion class:\n",
      " - Joy: 674 (24%)\n",
      " - Sadness: 878 (32%)\n",
      " - Surprise: 839 (30%)\n",
      " - Fear: 1611 (58%)\n",
      " - Anger: 333 (12%)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X: {X_train.shape}')\n",
    "print(f'Shape of y: {y_train.shape}')\n",
    "print(f'Number of positives per emotion class:')\n",
    "_ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373f821-e149-4ef0-ab2b-f069db413e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c5e0b62-3759-4883-a5a8-0e7e41b3c5e8",
   "metadata": {},
   "source": [
    "## Define the model \n",
    "\n",
    "We define a simple neural network model with 1 hidden layer for this task. This can be made arbitrarily more complex, eg. by experimenting with the types of inputs and layers, layer size, depth and regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5fd614-49e3-4833-8925-e4ce83ff35f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da018344-f73d-4f53-a6d0-64e4548c0eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45571c77-77d9-442a-ac58-3d7b95e6de4c",
   "metadata": {},
   "source": [
    "## Define the optimisation parameters\n",
    "\n",
    " To perform multilabel classification, we use binary cross entropy with logits. See [here](https://discuss.pytorch.org/t/is-there-an-example-for-multi-class-multilabel-classification-in-pytorch/53579/6) for a motivation. Here, one can explore different optimizers, regularisation levels, learning rates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53d60fa-bb19-4229-9470-19ef42188f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc32f06-bd17-455c-95e4-ea400a7dd8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "614cf6d1-f347-4f8f-a2a5-f1b749031c64",
   "metadata": {},
   "source": [
    "## Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403e494e-305d-4c7c-ad06-7c09bbbbb1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.682\n",
      "Epoch 100: Loss: 0.592\n",
      "Epoch 200: Loss: 0.573\n",
      "Epoch 300: Loss: 0.568\n",
      "Epoch 400: Loss: 0.565\n",
      "Epoch 500: Loss: 0.563\n",
      "Epoch 600: Loss: 0.561\n",
      "Epoch 700: Loss: 0.56\n",
      "Epoch 800: Loss: 0.559\n",
      "Epoch 900: Loss: 0.558\n"
     ]
    }
   ],
   "source": [
    "# Train for a set number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4aab9-a465-4e88-bd42-1a15a30dca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4152b82d-cc7d-42ac-a09d-59e227bbb7cb",
   "metadata": {},
   "source": [
    "## Get class predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e5a2b-cc42-4b2b-ae05-ac98820eba14",
   "metadata": {},
   "source": [
    "The model outputs logits to coordinate with the BCE. We use a sigmoid transformation to obtain a score in the range of (0,1). We need to define a classification threshold to obtain a binary prediction from the real-valued model output. This can be determined based on the validation data, and may be different for each emotion. Given the imbalance in the data, we set it slightly lower than 0.5 (the standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75eb754-c414-44d5-9621-08fa8b068965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3654a41-ae1b-4255-b9b2-63fa89d36486",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_predictions(X_val_t, model, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb71c6-ecc9-4e0c-83c7-317c4c5508d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5211feff-646a-4421-abc2-6ddc45a0d4cc",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60417e-6e9f-4047-81e6-f88e189d15d3",
   "metadata": {},
   "source": [
    "We evaluate the model based on the micro- and macro-averaged F1 score. The former aggregates the metrics at the per-sample level, whereas the latter does it at the per-class level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf5638e-233f-4ffe-92d0-dce910482d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_val, y_pred):\n",
    "    for average in ['micro', 'macro']:\n",
    "        recall = recall_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        precision = precision_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "        print(f'{average.upper()} recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f87015-27ac-4849-a80e-fea14d9289e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICRO recall: 0.358, precision: 0.5431, f1: 0.4315\n",
      "MACRO recall: 0.2, precision: 0.1086, f1: 0.1408\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92568bcf-96ac-4cfd-b5d3-cd745c4548a8",
   "metadata": {},
   "source": [
    "The results show that the macro-averaged F1 is much lower than the micro-averaged score. This indicates that the model might be performing poorly on some of the classes. Below, we evaluate each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d24d69d-1b8e-481f-af7f-ed1cb9a691ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_class(y_val, y_pred):\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f'*** {emotion} ***')\n",
    "    \n",
    "        recall = recall_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        precision = precision_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        f1 = f1_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "        print(f'recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c184d9b7-d8f9-44e8-8e6c-99f56f7c8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Joy ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** Sadness ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** Surprise ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** Fear ***\n",
      "recall: 1.0, precision: 0.5431, f1: 0.7039\n",
      "\n",
      "*** Anger ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b95ad1-a6b0-4a92-b409-41d65e801ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceac1d52-f2dc-43c2-a3da-cd34c782dacb",
   "metadata": {},
   "source": [
    "## Weighing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50959010-9160-4879-83f1-0726dd2ed0bb",
   "metadata": {},
   "source": [
    "We can see that the model performs well on the \"fear\" class, which is the most common, but poorly on all others, classifying all samples as negative. One way to address this is by assigning weights to the different classes to increase the effect of samples from rare classes. For example, the below snippet can be used to calculate weights based on their relative frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8eafd78-df3c-4ccf-b1e4-75aba7cb25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = y_train.sum(axis=0)/y_train.sum()\n",
    "weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed511d0-1dd9-4e01-a640-84aef5732f7b",
   "metadata": {},
   "source": [
    "These weights can then be assigned to the loss function for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f442358-0a91-4c69-add6-82234597aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.878\n",
      "Epoch 100: Loss: 0.863\n",
      "Epoch 200: Loss: 0.856\n",
      "Epoch 300: Loss: 0.851\n",
      "Epoch 400: Loss: 0.846\n",
      "Epoch 500: Loss: 0.84\n",
      "Epoch 600: Loss: 0.835\n",
      "Epoch 700: Loss: 0.829\n",
      "Epoch 800: Loss: 0.823\n",
      "Epoch 900: Loss: 0.817\n",
      "\n",
      "\n",
      "EVALUATION\n",
      "\n",
      "MICRO recall: 0.7443, precision: 0.4441, f1: 0.5563\n",
      "MACRO recall: 0.6461, precision: 0.3981, f1: 0.4785\n",
      "\n",
      "PER CLASS BREAKDOWN\n",
      "\n",
      "*** Joy ***\n",
      "recall: 0.2581, precision: 0.2963, f1: 0.2759\n",
      "\n",
      "*** Sadness ***\n",
      "recall: 0.8857, precision: 0.3735, f1: 0.5254\n",
      "\n",
      "*** Surprise ***\n",
      "recall: 0.7742, precision: 0.4444, f1: 0.5647\n",
      "\n",
      "*** Fear ***\n",
      "recall: 1.0, precision: 0.5431, f1: 0.7039\n",
      "\n",
      "*** Anger ***\n",
      "recall: 0.3125, precision: 0.3333, f1: 0.3226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model \n",
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )\n",
    "\n",
    "# Define training parameters\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "\n",
    "# Evaluate\n",
    "print('\\n\\nEVALUATION\\n')\n",
    "evaluate(y_val, y_pred)\n",
    "\n",
    "print('\\nPER CLASS BREAKDOWN\\n')\n",
    "evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdf267-544a-4a7e-9527-709e9d63c2e6",
   "metadata": {},
   "source": [
    "Using this approach, we can see that the model performs much better on average (and particularly on the less common classes), even though the final training error is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab53d9-5402-49f8-8a4f-9407298a4091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
