{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the two files into pandas DataFrames\n",
    "file_one_path = 'track_a_gold/pred_amh.csv'  # Adjust with your file path\n",
    "file_two_path = 'track_a/pred_amh.csv'  # Adjust with your file path\n",
    "\n",
    "df_pred = pd.read_csv(file_one_path)\n",
    "df_true = pd.read_csv(file_two_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation.ipynb \u001b[1m\u001b[36mtrack_a\u001b[m\u001b[m          track_a.zip      \u001b[1m\u001b[36mtrack_a_gold\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yimam/anaconda3/envs/nlpnweb/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yimam/anaconda3/envs/nlpnweb/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yimam/anaconda3/envs/nlpnweb/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Extract the list of emotion labels\n",
    "emotions = ['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "\n",
    "# Calculate precision, recall, and F1 for each emotion (macro)\n",
    "precision_scores = precision_score(df_true[emotions], df_pred[emotions], average=None)\n",
    "recall_scores = recall_score(df_true[emotions], df_pred[emotions], average=None)\n",
    "f1_scores = f1_score(df_true[emotions], df_pred[emotions], average=None)\n",
    "\n",
    "# Calculate macro and micro averages\n",
    "macro_precision = precision_score(df_true[emotions], df_pred[emotions], average='macro')\n",
    "macro_recall = recall_score(df_true[emotions], df_pred[emotions], average='macro')\n",
    "macro_f1 = f1_score(df_true[emotions], df_pred[emotions], average='macro')\n",
    "\n",
    "micro_precision = precision_score(df_true[emotions], df_pred[emotions], average='micro')\n",
    "micro_recall = recall_score(df_true[emotions], df_pred[emotions], average='micro')\n",
    "micro_f1 = f1_score(df_true[emotions], df_pred[emotions], average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per emotion: [0. 0. 0. 0. 0. 0.]\n",
      "Recall per emotion: [0. 0. 0. 0. 0. 0.]\n",
      "F1 Score per emotion: [0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Macro Precision: 0.0\n",
      "Macro Recall: 0.0\n",
      "Macro F1: 0.0\n",
      "\n",
      "Micro Precision: 0.0\n",
      "Micro Recall: 0.0\n",
      "Micro F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"Precision per emotion:\", precision_scores)\n",
    "print(\"Recall per emotion:\", recall_scores)\n",
    "print(\"F1 Score per emotion:\", f1_scores)\n",
    "\n",
    "print(\"\\nMacro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "\n",
    "print(\"\\nMicro Precision:\", micro_precision)\n",
    "print(\"Micro Recall:\", micro_recall)\n",
    "print(\"Micro F1:\", micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR AMH and DEU only [header is different]\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('track_b/pred_amh.csv')\n",
    "\n",
    "# Set the specified emotion columns to 0\n",
    "emotion_columns = ['Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df[emotion_columns] = 0\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('track_b/pred_amh.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For English\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('track_b/pred_eng.csv')\n",
    "\n",
    "# Set the specified emotion columns to 0\n",
    "emotion_columns = ['Anger','Fear','Joy','Sadness','Surprise']\n",
    "df[emotion_columns] = 0\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('track_b/pred_eng.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpnweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
